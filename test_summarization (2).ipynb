{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecs0rqPgyK6r"
      },
      "outputs": [],
      "source": [
        "# Install Required Libraries\n",
        "\n",
        "\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install rouge-score\n",
        "#We start by installing the necessary libraries. transformers is for accessing pre-trained models, torch is the backend for these models, datasets is for loading and evaluating datasets, and rouge-score is for evaluating the quality of the summaries.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCTXp0TBFQUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Model and Tokenizer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "#We load the T5 model and its tokenizer from Hugging Faceâ€™s model repository. The t5-small variant is used here for simplicity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDvlI4CMySjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the CNN/DailyMail dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "#Explanation: We load the CNN/DailyMail dataset, which is commonly used for text summarization tasks. The load_dataset function fetches the dataset and prepares it for use."
      ],
      "metadata": {
        "id": "JbtWX2iEDMQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the Dataset\n",
        "\n",
        "\n",
        "# Display the first example from the training set\n",
        "print(dataset['train'][0])\n",
        "#We explore the dataset by displaying the first example from the training set. This helps us understand the structure of the data.\n"
      ],
      "metadata": {
        "id": "gGBaNLwIDfJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the Data for Summarization\n",
        "\n",
        "\n",
        "# Extract the article and summary\n",
        "article = dataset['train'][0]['article']\n",
        "summary = dataset['train'][0]['highlights']\n",
        "# Tokenize the inputs\n",
        "inputs = tokenizer.encode(\"summarize: \" + article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "#We extract the article and its corresponding summary from the dataset. Then, we tokenize the article to prepare it for summarization.\n"
      ],
      "metadata": {
        "id": "zIAC-DRvD1P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the Summary\n",
        "summary_ids = model.generate(inputs, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(generated_summary)\n",
        "# We generate the summary using the model and print it. The parameters control the length and quality of the summary.\n"
      ],
      "metadata": {
        "id": "ZL5rDfLsEYN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "6L8Xok93EiND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Function for Summarization\n",
        "\n",
        "\n",
        "def summarize_text(text):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# We create a function summarize_text that takes a text input and returns its summary. This function encapsulates the summarization process, making it reusable.\n"
      ],
      "metadata": {
        "id": "9Hj8DOFIFB2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Processing for Multiple Articles\n",
        "\n",
        "\n",
        "articles = [\"Article 1 text\", \"Article 2 text\", \"Article 3 text\"]\n",
        "summaries = [summarize_text(article) for article in articles]\n",
        "for i, summary in enumerate(summaries):\n",
        "    print(f\"Summary {i+1}: {summary}\")\n",
        "\n",
        "# We process multiple articles in a batch. This is useful for summarizing a collection of articles at once. We loop through the articles, generate summaries, and print them.\n",
        "\n"
      ],
      "metadata": {
        "id": "vVArwqAUFSMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GfCI4HHQJg83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save summaries to a file\n",
        "with open('summaries.json', 'w') as f:\n",
        "    json.dump(summaries, f)\n",
        "\n",
        "# Load summaries from a file\n",
        "with open('summaries.json', 'r') as f:\n",
        "    loaded_summaries = json.load(f)\n",
        "print(loaded_summaries)\n",
        "\n",
        "# We save the generated summaries to a JSON file and load them back. This is useful for storing the summaries for future reference or further processing.\n"
      ],
      "metadata": {
        "id": "KiocirLxFZms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}