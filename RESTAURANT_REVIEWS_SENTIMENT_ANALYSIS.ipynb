{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMBk5Tg8CGSE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fBHZHDWbCZmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data=pd.read_csv('/content/Restaurant_Reviews.tsv',sep=\"\\t\")"
      ],
      "metadata": {
        "id": "3Xog6J6jF0py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "TEUrjlwJLjGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "yOyFDf8Yr9Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "ZtwxqHfI8YxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are checking weather null values are there or not  if null values are there it should be replaced with 0\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "FA3GEevq8Zij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking  weather data set is balanced or imbalanced\n",
        "data['Liked'].value_counts()\n",
        "# data set is balanced and model will give importance  to both the classes equal priority"
      ],
      "metadata": {
        "id": "2JyAjntc9yjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the length of characters\n",
        "\n",
        "# apply function gives in each review how many number of characters are there  length of it\n",
        "data['character_count']=data['Review'].apply(len)"
      ],
      "metadata": {
        "id": "kjnx-MOC-ssP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count gives length of characters in each  record"
      ],
      "metadata": {
        "id": "7Yn47bgVEb91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['word_count']=data['Review'].apply(lambda x: len(str(x).split()))\n",
        "# here apply function  to each row  each sentence of review will be converted to string and after that  it will be splitted   in to\n",
        "# individual words  and finding length of all splitted words\n"
      ],
      "metadata": {
        "id": "FVRfqJriFj5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "9nzigzSlcQes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to find out how many number of sentences in  each review  we have to use nltk library\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#NLTK (Natural Language Toolkit) is a popular Python library used for natural language processing (NLP).\n",
        "#It provides various utilities and tools for processing text, such as tokenization, stemming, lemmatization, part-of-speech tagging\n",
        "# punkt is a tokenizer  it is a model that splits the total sentence into small tokens means parts\n"
      ],
      "metadata": {
        "id": "ThrLSC0McqkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to find the total count of  sentences in each review\n",
        "# by using sentence toke\n",
        "data['count_of_sentences']=data['Review'].apply(lambda x:  len(nltk.sent_tokenize(str(x))))"
      ],
      "metadata": {
        "id": "wnpGjo2bCOg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yck-T_XxFryL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "JppDfW1xFcTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for positive and negative reviews how many characters are there"
      ],
      "metadata": {
        "id": "ofhJgWSLFuWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for positive reviews character count  and  average character count\n",
        "data[data['Liked']==1]['character_count'].mean()\n",
        "# average"
      ],
      "metadata": {
        "id": "-CMK4c89H6Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[data['Liked']==0]['character_count'].mean()"
      ],
      "metadata": {
        "id": "QWyULH79JbJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "O5kZdL5tUKW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing all non alphabetical characters\n",
        "review=data['Review'][0]"
      ],
      "metadata": {
        "id": "FiyNoEGTWfov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating pattern such that  in a sentence that alphabets should be there means it will create pattern as per that  based on that\n",
        "# sentences will be modified\n",
        "# sub is meathod  in which to replace"
      ],
      "metadata": {
        "id": "L2Mzc7c_hLcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review=re.sub('[^a-zA-Z]',\" \",data['Review'][0])\n",
        "# all non alphabetichal characters are removed"
      ],
      "metadata": {
        "id": "DSDk-G9CjlaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review=review.lower()"
      ],
      "metadata": {
        "id": "cOVK46-hmGMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "GMxTD6tombJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "all_stopwords=stopwords.words('english')"
      ],
      "metadata": {
        "id": "CasUjKHGnW28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_stopwords"
      ],
      "metadata": {
        "id": "-_L7tcR1o_LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop words\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "3EFddCqHpFg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_stopwords=stopwords.words('english')"
      ],
      "metadata": {
        "id": "U6fn_sa9vRLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_stopwords"
      ],
      "metadata": {
        "id": "3FQCBN8fveyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review=review.split()"
      ],
      "metadata": {
        "id": "-gr9ecWOGHAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews=[word for word in  review if word not in set(total_stopwords)]"
      ],
      "metadata": {
        "id": "51inHGhKGLVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews"
      ],
      "metadata": {
        "id": "DIYn7hW2Gaam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# here reducing stop words  will reduce dataset size\n",
        "# reducing stop words  training time also  thats why reducing stop words\n",
        "# because of removing stop words text data meaning is changing in that case we cant remove stop words\n",
        "# we should not remove stop words in sentiment analysis  which helps in sentiment analysis\n"
      ],
      "metadata": {
        "id": "J6SpTDNsBhK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "#converting the words with 'ing' forms and 'es'  inflicted words in to their base words\n"
      ],
      "metadata": {
        "id": "ksH5BXY1-kc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are using stemming algorithm that is porter stemmer to convert the inflicted words in to base words these words are originated\n",
        "# from which base word\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "0xEmYi95Nxd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps=PorterStemmer()"
      ],
      "metadata": {
        "id": "KEav4h0FPB55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review1=[ps.stem(word)for word  in reviews ]\n",
        "review1"
      ],
      "metadata": {
        "id": "PfjiIYIePH9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review1=\" \".join(review1)"
      ],
      "metadata": {
        "id": "AdM4NXhuRZGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review1"
      ],
      "metadata": {
        "id": "b_hjXAIZR6jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XGQ2eTiuc3sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review1"
      ],
      "metadata": {
        "id": "oiwjjVtBXiKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# below stop words should not be removed  if they are removed sentiment will change\n",
        "# we are removing these stop words from total list\n",
        "# \"^ this symbol shows that   if any words not matching a-z and A-Z  THEN THOSE WORDS  are replaced with \" \" \"\n",
        "import re\n",
        "custom_stopwords={'don',\"don't\",\"aren''t\",\"couldn\",\"couldn't\",'didn',\"didn't\",\n",
        "                  'doesn',\"doesn't\",\"hadn\", \"hadn't\",\"hasn\",\"hasn't\",\"haven\",\n",
        "                  \"haven't\",'isn',\"isn't\",\"ma\",'mightn',\"mightn't\",'mustn',\"mustn't\",\n",
        "                  'needn',\"needn't\",\"shan\", \"shan't\",\"no\",\"nor\",\"not\",\"shouldn\",\"shouldn't\",\n",
        "                \"wasn\",\"wasn't\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\"}\n",
        "\n",
        "corpus=[]\n",
        "ps=PorterStemmer()\n",
        "stop_words=set(stopwords.words('english'))-custom_stopwords\n",
        "for i in range(len(data)):\n",
        "  review=re.sub(\"[^a-zA-Z]\",\" \",data['Review'][i])\n",
        "  review=review.lower()\n",
        "  review=review.split()\n",
        "  review=[ps.stem(word) for word in review if word not in stop_words]\n",
        "  review=\" \".join(review)\n",
        "  corpus.append(review)\n",
        "\n"
      ],
      "metadata": {
        "id": "OPSsfOLnWR2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "-saJXHTAdGRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['processed_text']=corpus\n",
        "data"
      ],
      "metadata": {
        "id": "xKztn3EPkXpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word cloud is  the pictorial representation of most occured key words wordcloud\n",
        "from  wordcloud  import WordCloud\n",
        "\n"
      ],
      "metadata": {
        "id": "q7Oa6qyvoQy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "BmehgkjkqbbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc=WordCloud(width=500,height=300,background_color='White')"
      ],
      "metadata": {
        "id": "JYxsphUJrTSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for  positive reviews frequency of occurence of key words\n",
        "# each and every string will be concatenated with  different separators\n",
        "\n",
        "pos=wc.generate(data[data['Liked']==1]['processed_text'].str.cat(sep=\" \"))"
      ],
      "metadata": {
        "id": "zTqhdw-HsVbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(pos)"
      ],
      "metadata": {
        "id": "bQBvSz3N0Ulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word cloud for negative reviews\n",
        "neg =wc.generate(data[data['Liked']==0]['processed_text'].str.cat(sep=\" \"))"
      ],
      "metadata": {
        "id": "8sQzXaQY1mQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(neg)"
      ],
      "metadata": {
        "id": "zO6xPkgQ11PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we cant train ml models   with  text data\n",
        "# by using one hot encoding or bag of words we can  convert text data in to numerical vectors\n",
        "#"
      ],
      "metadata": {
        "id": "MoICOe2T2AYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text  import CountVectorizer\n",
        "# count vectorizer is applicable to 1500 features"
      ],
      "metadata": {
        "id": "UKQYBxWW4NBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv=CountVectorizer(max_features=1500)\n",
        " # max_features means it will include more number of words to be  included in vocabulary  means most frequently occured words in corpus\n"
      ],
      "metadata": {
        "id": "CWxD4Hnl69pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "SgBXVbgv7iqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "Q8nqPp_g8ude"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "LjUd1yvF9bbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=data['Liked']"
      ],
      "metadata": {
        "id": "iOeHo2vN9hra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "54DljbOj-ES5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "MB1CPOWn-KRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=65)"
      ],
      "metadata": {
        "id": "9mVe8ttp-YVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "wt-yPspV-nrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gn=GaussianNB()"
      ],
      "metadata": {
        "id": "tOkEXviI_nC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g=gn.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "TBu1UP85_x1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=g.predict(x_test)"
      ],
      "metadata": {
        "id": "TDAgzF5j_9pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
      ],
      "metadata": {
        "id": "k358t3woAPJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "0OacqQlYAYHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test,y_pred)"
      ],
      "metadata": {
        "id": "3ZaExfESAkAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "V5nduqt0A1sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=LogisticRegression()\n"
      ],
      "metadata": {
        "id": "Cfa7K0RXBExG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lk=lr.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "7_qSDo_oBIXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predl=lk.predict(x_test)"
      ],
      "metadata": {
        "id": "el99HvOsBUTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_predl,y_test)\n"
      ],
      "metadata": {
        "id": "De2GWP32BgnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "i7Hb-I5iBnaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rc=RandomForestClassifier()\n"
      ],
      "metadata": {
        "id": "acjQs4AFCAlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand=rc.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "CfMna4FjCI8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predr=rand.predict(x_test)"
      ],
      "metadata": {
        "id": "S3hmL3TmCR02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_predr,y_test)"
      ],
      "metadata": {
        "id": "_Of-CkSlCbHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "0c7wqO6YCi_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GBC=GradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "b2OFwnpJCoG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GB=GBC.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "Z1-pSukuCqcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(GB.predict(x_test),y_test)"
      ],
      "metadata": {
        "id": "u3pfI9HACvTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n"
      ],
      "metadata": {
        "id": "I3j83JbgDDS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compared with all other models random forest has performed better we are going to save it for the restaurant reviews\n",
        "joblib.dump(rand,'restaurant_review_model.pkl')"
      ],
      "metadata": {
        "id": "y09UniVBDchs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}