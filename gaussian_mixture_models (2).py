# -*- coding: utf-8 -*-
"""gaussian mixture models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1btSdrrYr_g2gSk8x00PgzPFnr0M3y6CV
"""

import pandas as pd

data=pd.read_csv(r'https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/002/856/original/scaler_clustering.csv')

data

# checking for null values

data.isna().sum()

# creating class
def pk(ctc):
  if ctc>1000000 and ctc<=2000000 :
    return 'A'
  elif ctc>2000000 and ctc<=3000000:
    return 'B'
  elif ctc>3000000 and ctc<=4000000:
    return 'c'
  elif ctc>4000000 and ctc<=6000000:
    return 'C'
  elif ctc>6000000 and ctc<=8000000:
    return 'D'
  elif ctc>8000000 and ctc<=10000000:
    return 'E'
  else:
    return 'F'
from sklearn.impute import SimpleImputer



s1=s.fit(data[['orgyear']])
data['orgyear']=s1.transform(data[['orgyear']])

data['orgyear']

data['job_position']=data['job_position'].fillna(data['job_position'].mode()[0])

# creating class

data.info()

data.describe()

data.duplicated()

data.head(5)

# manual clustering
# creating designation flag and insights
data['job_position'].unique()

data['job_position']=data['job_position'].map({'Other':0,'FullStack Engineer':1,'FullStack Engineer':2,'Backend Engineer':3,'Web / UI Designer':4,'Azure data Factory':5,
                                           'Android Application developer':6,})

data.groupby('job_position')['ctc'].mean()
# thes are average ctc values

data['ctc'].unique()

data1=data.copy()

# creating class flag and insights based on ctc by keeping labels

def lk(ctc):
  if ctc>1000000 and ctc<=2000000 :
    return 'vey_less_ctc'
  elif ctc>2000000 and ctc<=3000000:
    return 'less_ctc'
  elif ctc>3000000 and ctc<=4000000:
    return 'average_ctc'
  elif ctc>4000000 and ctc<=6000000:
    return 'good_ctc'
  elif ctc>6000000 and ctc<=8000000:
    return 'better_ctc'
  elif ctc>8000000 and ctc<=10000000:
    return 'second_best_ctc'
  else:
    return 'best_ctc'

data['class']=data['ctc'].apply(lk)
import seaborn as sns
# imported seaborn package in that many visualization functions are there

# class is visualized  based on histogram
sns.histplot(x=data['class'],data=data)
import matplotlib.pyplot as plt
plt.xticks(rotation=90)

# here best ctc is more

# creating tier  flag  and insights
def pk(ctc):
  if ctc>1000000 and ctc<=2000000 :
    return 'A'
  elif ctc>2000000 and ctc<=3000000:
    return 'B'
  elif ctc>3000000 and ctc<=4000000:
    return 'c'
  elif ctc>4000000 and ctc<=6000000:
    return 'C'
  elif ctc>6000000 and ctc<=8000000:
    return 'D'
  elif ctc>8000000 and ctc<=10000000:
    return 'E'
  else:
    return 'F'

data['ctc']=data['ctc'].apply(pk)

l=data['ctc'].value_counts()
l

import seaborn as sns
sns.histplot(x=data['ctc'])
# the frequency of occurence of f category is more means more people are having ctc  greater than 10000000

# categorical values will not be understood by machine learning models
# based on label encoder  all categorical values are transformed in to numerical values

from sklearn.preprocessing import LabelEncoder

# instance of label encoder is created
lr=LabelEncoder()

# label encoder instance is fitted to column company_hash,email_hash,job_position
data1['company_hash']=lr.fit_transform(data[['company_hash']])

data1['email_hash']=lr.fit_transform(data['email_hash'])

data1['job_position']=lr.fit_transform(data['job_position'])

data1

# bigger values and smaller values are scaled in between 0 to 1 or -1 to 1  by using minmaxscaler
from sklearn.preprocessing import MinMaxScaler

m=MinMaxScaler()
import pandas as pd
# instance of min max scaler is created

data1=pd.DataFrame(m.fit_transform(data1),columns=data1.columns)

data1.isna().sum()
data1['orgyear']=data1['orgyear'].fillna(data1['orgyear'].mode()[0])
data1.isna().sum()

# unsupervised learning
# elbow method
# KMeans algorithm was imported
g={}
from sklearn.cluster import KMeans
for i in range(1,11):
  model=KMeans(n_clusters=i)
  c=model.fit(data1)
  g[i]=c.inertia_

from yellowbrick.cluster import KElbowVisualizer
vis=KElbowVisualizer(KMeans(),k=(1,10))
vi=vis.fit(data1)
vi.show()
# KELbowVisualizer was imported

#3 clusters are more desirable
# hierarchial clustering
data1

from sklearn.cluster import AgglomerativeClustering
# Agglomerative clustering was imported

# n_clusyers is a hyperparameter if it increases wcss value decreases
ag=AgglomerativeClustering(n_clusters=3)

p=ag.fit(data1.head(1500))

p

#Business Insights:
#Talent Profiling: Gain insights into the types of learners attracted to Scalers courses based on their job profiles and company affiliations.
#Company Preferences: Identify which companies employees are more inclined towards upskilling through Scaler's courses.
#Course Effectiveness: Evaluate the effectiveness of Scaler's courses by examining learner performance within different clusters.
#Market Segmentation: Segment the market of potential learners based on their characteristics, allowing for targeted marketing strategies.

# Recommendations:
#Tailored Course Offerings: Develop specialized courses or modules tailored to the needs and preferences of specific learner clusters.
#Strategic Partnerships: Forge partnerships with companies whose employees exhibit a high propensity for upskilling through Scaler.
#Personalized Learning Paths: Offer personalized learning paths or recommendations based on the learner cluster they belong to, maximizing engagement and course completion rates.
#Continuous Improvement: Continuously refine course content and delivery based on insights gained from clustering analysis to ensure relevance and effectiveness.

from sklearn.cluster import r